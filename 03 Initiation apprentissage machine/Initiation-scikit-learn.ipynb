{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à l'apprentissage machine\n",
    "\n",
    "Nous commençons l'apprentissage machine!\n",
    "\n",
    "Ce qu'il faut savoir, c'est qu'il existe 3 grandes catégories de ML (machine learning).\n",
    "- Apprentissage supervisé\n",
    "- Apprentissage non-supervisé\n",
    "- Apprentissage par renforcement\n",
    "\n",
    "Dans ce volet, seul l'apprentissage supervisé sera couvert.\n",
    "\n",
    "## Type d'apprentissage\n",
    "Voici une explication courte sur les différents type d'apprentissage.\n",
    "### Apprentissage supervisé\n",
    "On cherche à classifier chaque donnée (comprendre chaque ligne de notre jeu de données) à une étiquette. Prenons un exemple : j'ai un ensemble de données qui peuvent être classifiées soit pour la classe A ou la classe B. L'algorithme va tenter de séparer les données selon ces classes.\n",
    "\n",
    "Les classiques sont :\n",
    "- [Régression logistique](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [SVM](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "- [KNN](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)\n",
    "- [Arbre de décision](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    "Et plusieurs autres... [apprentissage supervisé](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "\n",
    "### Apprentissage non-supervisé\n",
    "On cherche, à partir des données, à créer des classes SANS avoir les étiquettes. Ainsi, sans connaître le nombre d'étiquettes, nous donnons aux algorithmes d'apprentissage non-supervisé (clustering) un nombre de regroupements à faire et le calcul se fera. On essaie avec 2, 3, 4 et plus jusqu'à ce que ce soit satisfaisant. Quand est-ce que ce sera satisfaisant? Il faut avoir une connaissance de nos données, idéalement. Sinon, la visualisation des regroupements peuvent aider le scientifique de données à voir ce qui semble ressortir.\n",
    "\n",
    "### Apprentissage par renforcement\n",
    "Ici, c'est complètement différent des deux derniers types d'apprentissage. C'est inspiré de l'apprentissage que ferait un bébé. Par exemple, il tente quelque chose, puis il reçoit une récompense (lance un cri et reçoit du lait en récompense) ou une punition (tente de marcher, mais tombe se fait mal). Dans le cas des machines, l'exemple le plus simple est un robot qui se déplace dans un labyrinthe. Ce robot contient une liste d'actions : avancer d'une case, tourner à gauche. Son but est d'arriver à la sortie du labyrinthe. Sa récompense pour avoir trouvé la sortie est de, disons, 500 points. Sa pénalité est le temps qu'il prend pour sortir du labyrinthe. Ainsi, son objectif est de sortir le plus rapidement possible. Afin d'éviter qu'il reste pris indéfiniment pendant une partie, le temps est limité, sinon, c'est la mort et on doit recommencer la partie.\n",
    "\n",
    "De manière itérative, le robot apprendra les actions qu'il peut faire et le meilleur moyen de trouver la sortie.\n",
    "\n",
    "Cette technique est relativement jeune, il y a beaucoup de recherches qui se font dans ce domaine, dont \n",
    "- les robots\n",
    "- des algorithmes qui apprennent à jouer à des jeux plus ou moins complexes (échec, go, StarCraft, etc.)\n",
    "- les algorithmes de proposition d'items comme certains sites d'achats en ligne\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Certes, il y a énormément de fonctions mathématiques et statistiques. C'est la magie de l'intelligence artificielle : il n'y en a pas, ce ne sont que des statistiques en dessous et beaucoup, mais beaucoup de données!\n",
    "\n",
    "Vous pouvez prendre le temps de tenter de comprendre chacun des algorithmes qui existent dans Scikit-Learn à l'aide de leurs exemples, de Wikipedia, de Youtube ou même de plusieurs forums ou discord. Ce n'est pas l'information qui manque!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afin d'avoir un jeu de données sur lequel jouer, nous allons installer un paquet supplémentaire : scikit-learn. \n",
    "# Vous savez déjà comment installer un paquet. ;)\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "# Mais vous savez, installer les paquets un à un devient lourd et ce n'est pas du tout pratique\n",
    "# pour des installations automatisées en production...\n",
    "# Il y a moyen de faire des installations plus simples et plus robustes avec pip. Vous pouvez observer le fichier \"requirements.txt\".\n",
    "# Vous pouvez installer les paquets avec la commande \"pip install -r requirements.txt\".\n",
    "# Les paquets suivants vont nous aider pour la visualisation des données :\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9.0, 7.0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commençons par charger les données de l'apprentissage machine.\n",
    "data = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici une description du jeu de données :\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici la liste des noms des fleurs de notre jeu de données :\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afin de mieux comprendre les données, nous allons afficher les données.\n",
    "# Cette ligne crée une liste contenant toutes les paires possibles\n",
    "# entre les 4 mesures.\n",
    "# Par exemple : [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "pairs = [(i, j) for i in range(4) for j in range(i+1, 4)]\n",
    "\n",
    "# Utilisons cette liste de paires pour afficher les données, deux\n",
    "# mesures à la fois.\n",
    "# On crée une figure à plusieurs sous-graphes.\n",
    "fig, subfigs = plt.subplots(2, 3, tight_layout=True)\n",
    "colors = np.array([x for x in \"bgrcmyk\"])\n",
    "\n",
    "for (f1, f2), subfig in zip(pairs, subfigs.reshape(-1)):\n",
    "    subfig.scatter(data.data[:, f1], data.data[:, f2], cmap=plt.cm.Paired, color=colors[data.target].tolist())\n",
    "    subfig.set_xlabel(data.feature_names[f1])\n",
    "    subfig.set_ylabel(data.feature_names[f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il existe un grand nombre d'algorithmes d'apprentissage machine disponibles. sciki-learn en possède plus d'une centaine.\n",
    "# Ce n'est pas tous les algorithmes qui fonctionnent avec tous les jeux de données. Malheureusement, il faut soit très bien les connaître, \n",
    "# soit les tester. Tout en n'oubliant pas notre connaissance du jeu de données.\n",
    "# Voici une visualisation de plusieurs algorithmes d'apprentissage machine \n",
    "# (source : https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py):\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = 0.02 #On indique la grandeur du pas pour la recherche en grilles\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "# Chaque paramètre des algorithmes suivants s'appelle hyperparamètre. Ces hyperparamètres sont des paramètres qui déterminent \n",
    "# le comportement de l'algorithme de classification.\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "# Nous créons un jeu de données d'apprentissage.\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]\n",
    "\n",
    "# Ici, il s'agit du code pour faire l'entrainement et pour faire la visualisation.\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# On utilise la fonction \"iterate_over_datasets\" pour parcourir les jeux de données.\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # On fait un prétraitement de données, on sépare les données d'apprentissage et de test.\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Affichage des données\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # On utilise la fonction \"plot_dataset\" pour afficher les données d'apprentissage.\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # On utilise la fonction \"plot_dataset\" pour afficher les données de test.\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # On utilise la fonction \"iterate_over_classifiers\" pour parcourir les algorithmes d'apprentissage.\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # On utilise la fonction \"plot_decision_boundary\" pour afficher la frontière de décision [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # On affiche les résultats sous forme de couleurs.\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n",
    "\n",
    "        # On dessine les points d'apprentissage.\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # On dessine les points de test.\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            xx.max() - 0.3,\n",
    "            yy.min() + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "C'est l'heure de vous faire essayer les différents algorithmes! Vu le temps qui nous est imparti, nous ne regarderons qu'un seul algorithme : le KNN.\n",
    "\n",
    "Dans la cellule suivante, il y a beaucoup de code, ne vous inquiétez pas. Une majeure partie est pour la gestion des données et pour l'affichage.\n",
    "\n",
    "Le but de l'exercice est d'expérimenter les hyperparamètres pour trouver ceux qui permettront d'avoir le meilleur score de précision. Le score de précision (accuracy) est ce qui permet d'évaluer si notre modèle performe bien ou non. Plus le score est élevé, mieux c'est, généralement... Il faut faire attention au surentraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Voici les hyperparamètres avec lesquels vous pouvez jouer pour tenter d'améliorer le score.\n",
    "# Pour connaître ce que vous pouvez modifier, consultez la documentation de la fonction \"KNeighborsClassifier\".\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=nearest%20neighbors%20classification%20accuracy\n",
    "n_neighbors = 5\n",
    "algorithm = \"auto\"\n",
    "leaf_size = 30\n",
    "p = 2\n",
    "\n",
    "# À partir d'ici, vous n'avez pas à modifier le code.\n",
    "\n",
    "# On importe les données.\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Nous ne prenons que les 2 premiers features. Nous pourrions éviter cette lourde\n",
    "# sélection en utilisant un dataset de deux dimensions.\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.4, random_state=42)\n",
    "#X = iris.data[:, :2]\n",
    "#y = iris.target\n",
    "\n",
    "h = 0.02  # Grandeur du pas\n",
    "\n",
    "# On utilise la fonction \"make_cmap\" pour créer une palette de couleurs.\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "for weights in [\"uniform\", \"distance\"]:\n",
    "    # C'est ici que la magie se passe pour la création du modèle à créer.\n",
    "    # La première ligne est la création du modèle vide, non entraîné.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights, algorithm=algorithm, leaf_size=leaf_size, p=p)\n",
    "    # La deuxième ligne est l'entrainement du modèle.\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # On utilise la fonction \"plot_decision_boundary\" pour afficher la frontière de décision [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Ici est la seconde magie, on utilise la fonction \"predict\" pour prédire les valeurs de la frontière de décision.\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # La fonction \"score\" permet de calculer le score de précision du modèle.\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(f\"Modèle de \\\"{weights}\\\" a obtenu le score : {score}.\")\n",
    "\n",
    "    # On place le résultat dans un graphique en couleur.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # On dessine les points d'apprentissage.\n",
    "    sns.scatterplot(\n",
    "        x=X_train[:, 0],\n",
    "        y=X_train[:, 1],\n",
    "        hue=iris.target_names[y_train],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(f\"3-Class classification (k = {n_neighbors}, weights = {weights})\")\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Pour conclure le tout, vous pouvez expérimenter avec les autres algorithmes tel que montré au début du notebook dont la liste est ramené ici :\n",
    "- [Régression logistique](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [SVM](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "- [KNN](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)\n",
    "- [Arbre de décision](https://scikit-learn.org/stable/modules/tree.html#classification)\n",
    "\n",
    "Et les autres... [apprentissage supervisé](https://scikit-learn.org/stable/supervised_learning.html). ;)\n",
    "\n",
    "Vous pourrez y voir que le plus gros des entrainements se résume à instancier la classe de l'algorithme, à faire un entrainement avec `fit` puis une prédiction avec `predict` et/ou un `predict_proba`.\n",
    "\n",
    "Le plus gros du travail est :\n",
    "- Obtenir les données\n",
    "- Nettoyer les données pour les rendre exploitable\n",
    "- Trouver le bon algorithme d'apprentissage\n",
    "- Créer le modèle et trouver les bons hyperparamètres\n",
    "- Valider que l'entrainement apporte un gain significatif par rapport à une autre méthode plus classique de programmation. En effet, une simple logique d'affaire, une expression régulière, une analyse du besoin peut apporter de meilleur résultat en un temps plus rapide.\n",
    "- Déployer le modèle en production\n",
    "- Surveiller le modèle pour le moment où il ne répondra plus au besoin. Ce n'est pas une question de si, mais bien de quand...\n",
    "\n",
    "Lorsque le modèle ne répondra plus au besoin car les données ont changées, car le besoin d'affaire a changé, il faudra refaire tout le processus d'analyse, de préparation des données et d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d6a3aa1c9ecd92ae97edbdcf3e33df99c3e25412dd0280a36f286d4c187fbfe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fdemopython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
